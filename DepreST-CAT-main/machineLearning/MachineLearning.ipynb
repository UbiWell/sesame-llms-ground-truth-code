{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Zh3QdeIgNRp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Author: ML Tlachac, WPI\n",
    "#For DepreST-CAT, 2021\n",
    "#Modified this notebook to allow replacing ground truth values with LLM simulated data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import collections\n",
    "import operator\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import metrics\n",
    "from statistics import mean\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA, KernelPCA, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZT_qr4lwqygD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "drive_base = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711318997268,
     "user": {
      "displayName": "Varun Mishra",
      "userId": "16035457180629638947"
     },
     "user_tz": 240
    },
    "id": "MpaDemSpgNRq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to completely replace real scores with GPT simulated\n",
    "def replace_gad_phq_df(df, replace_ids, split):\n",
    "\n",
    "    print(\"replacing orginal scores with GPT GAD and PHQ scores\")\n",
    "    with open(drive_base + \"outputs/gpt-4-gad-given-phq\", 'rb') as file:\n",
    "        chat_gpt_scores_dict_gad = pickle.load(file)\n",
    "\n",
    "    with open(drive_base + \"outputs/gpt-4-phq-given-gad\", 'rb') as file:\n",
    "        chat_gpt_scores_dict_phq = pickle.load(file)\n",
    "\n",
    "    print(\"replacing in\", len(replace_ids))\n",
    "    for i in replace_ids:\n",
    "        gad_score = sum(chat_gpt_scores_dict_gad[i])\n",
    "        phq_score = sum(chat_gpt_scores_dict_phq[i])\n",
    "\n",
    "\n",
    "        if int(gad_score) >= split:\n",
    "            df.loc[df['id'] == i, 'gad7'] = 1\n",
    "        else:\n",
    "            df.loc[df['id'] == i, 'gad7'] = 0\n",
    "        if int(phq_score) >= split:\n",
    "            df.loc[df['id'] == i, 'phq9'] = 1\n",
    "        else:\n",
    "            df.loc[df['id'] == i, 'phq9'] = 0\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VCpFEn8ZqoEf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to replace real scores with GPT generated or SVR generated scores\n",
    "def replace_gad_phq_SVR_or_GPT(df, original_df, replace_ids, split, gpt=False):\n",
    "    data_split = 0.7\n",
    "    model_ids = replace_ids[: int(data_split*len(replace_ids))]\n",
    "    predict_ids = replace_ids[int(data_split*len(replace_ids)):]\n",
    "\n",
    "    if gpt:\n",
    "        print(f\"replacing original scores with GPT GAD and PHQ scores using GPT for {(1-data_split) * 100}% data\")\n",
    "        with open(drive_base + \"outputs/gpt-4-gad-given-phq\", 'rb') as file:\n",
    "            chat_gpt_scores_dict_gad = pickle.load(file)\n",
    "\n",
    "        with open(drive_base + \"outputs/gpt-4-phq-given-gad\", 'rb') as file:\n",
    "            chat_gpt_scores_dict_phq = pickle.load(file)\n",
    "\n",
    "    else:\n",
    "        print(f\"replacing original scores with GPT GAD and PHQ scores using SVR model {(1-data_split) * 100}% data\")\n",
    "        model_data = original_df[original_df['id'].isin(model_ids)]\n",
    "        x_data = np.array(model_data['gad7']).reshape(-1, 1)\n",
    "        y_data = np.array(model_data['phq9']).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        y_svr_model = SVR(kernel=\"rbf\", C=10, gamma=0.1, epsilon=.1)\n",
    "        x_svr_model = SVR(kernel=\"rbf\", C=10, gamma=0.1, epsilon=.1)\n",
    "        y_svr_model.fit(x_data, y_data)\n",
    "        x_svr_model.fit(y_data, x_data)\n",
    "\n",
    "    print(\"replacing in\", len(predict_ids))\n",
    "    for i in predict_ids:\n",
    "        if gpt:\n",
    "            new_gad_score = sum(chat_gpt_scores_dict_gad[i])\n",
    "            new_phq_score = sum(chat_gpt_scores_dict_phq[i])\n",
    "        else:\n",
    "            orig_gad = original_df.loc[df['id'] == i, 'gad7'].iloc[0]\n",
    "            orig_phq = original_df.loc[df['id'] == i, 'phq9'].iloc[0]\n",
    "            new_gad_score = round(x_svr_model.predict([[orig_phq]])[0])\n",
    "            new_phq_score = round(y_svr_model.predict([[orig_gad]])[0])\n",
    "\n",
    "        if int(new_gad_score) >= split:\n",
    "            df.loc[df['id'] == i, 'gad7'] = 1\n",
    "        else:\n",
    "            df.loc[df['id'] == i, 'gad7'] = 0\n",
    "\n",
    "        if int(new_phq_score) >= split:\n",
    "            df.loc[df['id'] == i, 'phq9'] = 1\n",
    "        else:\n",
    "            df.loc[df['id'] == i, 'phq9'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vOA1sKWwgNRr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to replace real scores with GPT generated or Linear Regression generated scores\n",
    "def replace_gad_phq_linear_model_or_GPT(df, original_df, replace_ids, split, gpt=False):\n",
    "    data_split = 0.7\n",
    "    model_ids = replace_ids[: int(data_split*len(replace_ids))]\n",
    "    predict_ids = replace_ids[int(data_split*len(replace_ids)):]\n",
    "\n",
    "    if gpt:\n",
    "        print(f\"replacing original scores with GPT GAD and PHQ scores using GPT for {(1-data_split) * 100}% data\")\n",
    "        with open(drive_base + \"outputs/gpt-4-gad-given-phq\", 'rb') as file:\n",
    "            chat_gpt_scores_dict_gad = pickle.load(file)\n",
    "\n",
    "        with open(drive_base + \"outputs/gpt-4-phq-given-gad\", 'rb') as file:\n",
    "            chat_gpt_scores_dict_phq = pickle.load(file)\n",
    "\n",
    "    else:\n",
    "        print(\"replacing original scores with GPT GAD and PHQ scores using linear model\")\n",
    "        model_data = original_df[original_df['id'].isin(model_ids)]\n",
    "        x_data = np.array(model_data['gad7'])\n",
    "        y_data = np.array(model_data['phq9'])\n",
    "\n",
    "        coefficients = np.polyfit(x_data, y_data, 1)  # Fit a first-degree (linear) polynomial\n",
    "\n",
    "\n",
    "        m = coefficients[0]\n",
    "        b = coefficients[1]\n",
    "        y_fit = m * x_data + b\n",
    "\n",
    "    print(\"replacing in\", len(predict_ids))\n",
    "    for i in predict_ids:\n",
    "\n",
    "        if gpt:\n",
    "            new_gad_score = sum(chat_gpt_scores_dict_gad[i])\n",
    "            new_phq_score = sum(chat_gpt_scores_dict_phq[i])\n",
    "        else:\n",
    "            orig_gad = original_df.loc[df['id'] == i, 'gad7'].iloc[0]\n",
    "            orig_phq = original_df.loc[df['id'] == i, 'phq9'].iloc[0]\n",
    "            new_gad_score = int((orig_phq - b)/m)\n",
    "            new_phq_score = int(m*orig_gad + b)\n",
    "\n",
    "        # print(gad_score, phq_score)\n",
    "\n",
    "\n",
    "        if int(new_gad_score) >= split:\n",
    "            df.loc[df['id'] == i, 'gad7'] = 1\n",
    "        else:\n",
    "            df.loc[df['id'] == i, 'gad7'] = 0\n",
    "\n",
    "        if int(new_phq_score) >= split:\n",
    "            df.loc[df['id'] == i, 'phq9'] = 1\n",
    "        else:\n",
    "            df.loc[df['id'] == i, 'phq9'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1711319169684,
     "user": {
      "displayName": "Varun Mishra",
      "userId": "16035457180629638947"
     },
     "user_tz": 240
    },
    "id": "Mg9VLdZTgNRr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = [\"phq9\", \"gad7\"]\n",
    "modelTypelist = [\"SVC\", \"kNN\", \"RF\", \"LR\", \"XG\"]\n",
    "splits = [5,6,7,8,9,10]\n",
    "drive_base = \"../../\"\n",
    "for split in splits:\n",
    "\n",
    "    for week in [2,4,8,16]:\n",
    "        data = pd.read_csv(drive_base + \"DepreST-CAT-main/features/featureSet\" + str(week) + \"weeksDepreST-CAT.csv\")\n",
    "        original_data = data.copy(deep=True)\n",
    "        print(data.shape)\n",
    "\n",
    "        #binary labels\n",
    "        d10 = []\n",
    "        g10 = []\n",
    "        for i in range(0, data.shape[0]):\n",
    "            if int(data.phq9[i]) >= split:\n",
    "                d10.append(1)\n",
    "            else:\n",
    "                d10.append(0)\n",
    "            if int(data.gad7[i]) >= split:\n",
    "                g10.append(1)\n",
    "            else:\n",
    "                g10.append(0)\n",
    "        data[\"phq9\"] = d10\n",
    "        data[\"gad7\"] = g10\n",
    "\n",
    "        print(data.shape)\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            #create lists to populate\n",
    "            flist = []\n",
    "            mlist = []\n",
    "            llist = []\n",
    "            featureList = []\n",
    "            wlist = []\n",
    "            slist = []\n",
    "            f1List = []\n",
    "            accuracyList = []\n",
    "            truePosList = []\n",
    "            trueNegList = []\n",
    "            falsePosList = []\n",
    "            falseNegList = []\n",
    "            predictions = []\n",
    "            rseed = []\n",
    "\n",
    "            for r in range(0, 100):\n",
    "\n",
    "                #train/test split\n",
    "                df_train, df_test = train_test_split(data, test_size=0.3, stratify=data[[\"phq9\", \"gad7\"]], random_state = r)\n",
    "                # creating data_copy to prevent any leakage\n",
    "                data_copy = data.copy(deep=True)\n",
    "                trainids = list(df_train[\"id\"])\n",
    "                testids = list(df_test[\"id\"])\n",
    "                # replacing scores only in train data\n",
    "                data_copy = replace_gad_phq_df(data_copy, trainids, split)\n",
    "                #for svr\n",
    "                # data_copy = replace_gad_phq_SVR_or_GPT(data_copy, original_data, trainids, split, False)\n",
    "\n",
    "                print(data.shape)\n",
    "                testdata = data_copy[data_copy['id'].isin(testids)]\n",
    "                print(testdata.shape)\n",
    "                traindata = data_copy[data_copy['id'].isin(trainids)]\n",
    "                print(traindata.shape)\n",
    "\n",
    "                #limit to features\n",
    "                testContent = testdata[testdata.columns[1:-2]]\n",
    "                print(testContent.shape)\n",
    "                trainContent = traindata[traindata.columns[1:-2]]\n",
    "                print(trainContent.shape)\n",
    "\n",
    "                #NEED TO SCALE BEFORE FEATURE SELECTION/REDUCATION\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                np_scaled = min_max_scaler.fit_transform(trainContent)\n",
    "                featureSubset = pd.DataFrame(np_scaled)\n",
    "                np_scaled2 =  min_max_scaler.transform(testContent)\n",
    "                testSubset = pd.DataFrame(np_scaled2)\n",
    "                print(featureSubset.shape)\n",
    "                print(testSubset.shape)\n",
    "\n",
    "                target = list(traindata[label])\n",
    "\n",
    "                featureDF = []\n",
    "                testDFs = []\n",
    "\n",
    "                nFeatureList = list(np.arange(1,5,1))\n",
    "                for numberOfFeatures in nFeatureList:\n",
    "                    pca = PCA(n_components=numberOfFeatures)\n",
    "                    pca = pca.fit(featureSubset)\n",
    "                    X_pca = pca.transform(featureSubset)\n",
    "                    pcaDF = pd.DataFrame(X_pca)\n",
    "                    pcaDF = pcaDF.assign(target = target)\n",
    "                    featureDF.append(pcaDF)\n",
    "                    testSubset2 = pca.transform(testSubset)\n",
    "                    testDFs.append(pd.DataFrame(testSubset2))\n",
    "\n",
    "\n",
    "                for f in range(0, len(featureDF)):\n",
    "                    print(f)\n",
    "\n",
    "                    train_phq9 = featureDF[f]\n",
    "                    X_test = testDFs[f]\n",
    "\n",
    "                    # upsampling\n",
    "                    #Count 1s and 0s\n",
    "                    ones = len(train_phq9.loc[train_phq9['target'] == 1])\n",
    "                    zeros = len(train_phq9.loc[train_phq9['target'] == 0])\n",
    "                    if ones >= zeros:\n",
    "                        majority = 1\n",
    "                        minority = 0\n",
    "                    else:\n",
    "                        majority = 0\n",
    "                        minority = 1\n",
    "\n",
    "\n",
    "                    # Upsample TrainingSet\n",
    "                    train_majority = train_phq9[train_phq9.target==majority]\n",
    "                    train_minority = train_phq9[train_phq9.target==minority]\n",
    "\n",
    "                    #print(\"train_majority =\"  + str(len(train_majority)))\n",
    "                    #print(\"train_minority =\"  + str(len(train_minority)))\n",
    "\n",
    "                    # Upsample minority class\n",
    "                    train_minority_upsampled = resample(train_minority,\n",
    "                                                     replace=True,     # sample with replacement\n",
    "                                                     n_samples=len(train_majority),    # to match majority class\n",
    "                                                     random_state=42) # reproducible results\n",
    "\n",
    "                    # Combine majority class with upsampled minority class\n",
    "                    train_phq9 = pd.concat([train_majority, train_minority_upsampled])\n",
    "\n",
    "                    #seperate features and target\n",
    "                    y_train = train_phq9[\"target\"]\n",
    "                    X_train = train_phq9.drop(columns = \"target\")\n",
    "\n",
    "                    for modelType in modelTypelist:\n",
    "\n",
    "                        #add data to lists\n",
    "                        llist.append(label)\n",
    "                        featureList.append(f +1)\n",
    "                        flist.append(\"PCA\")\n",
    "                        mlist.append(modelType)\n",
    "\n",
    "                        #chose model type\n",
    "                        if modelType == \"SVC\":\n",
    "                            clf = svm.SVC(random_state=r)\n",
    "                        elif modelType == \"RF\":\n",
    "                            clf = RandomForestClassifier(random_state=r)\n",
    "                        elif modelType == \"kNN\":\n",
    "                            clf = KNeighborsClassifier()\n",
    "                        elif modelType == \"LR\":\n",
    "                            clf = LogisticRegression(random_state=r)\n",
    "                        elif modelType ==\"XG\":\n",
    "                            clf = xgb.XGBClassifier(random_state=r)\n",
    "\n",
    "                        #train model and make predictions\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        y_pred = clf.predict(X_test)\n",
    "\n",
    "                        #evaluate model\n",
    "                        conf_mat = confusion_matrix(list(testdata[label]), y_pred)\n",
    "                        TN = conf_mat[0][0]\n",
    "                        TP = conf_mat[1][1]\n",
    "                        FP = conf_mat[0][1]\n",
    "                        FN = conf_mat[1][0]\n",
    "                        precision = TP/(TP+FP)\n",
    "                        sensitivity = TP/(TP+FN)\n",
    "                        f1 = (2*precision*sensitivity)/(precision + sensitivity)\n",
    "                        accuracy = (TP+TN)/(TN+TP+FP+FN)\n",
    "\n",
    "                        #populate lists with results\n",
    "                        f1List.append(f1)\n",
    "                        accuracyList.append(accuracy)\n",
    "                        truePosList.append(TP)\n",
    "                        trueNegList.append(TN)\n",
    "                        falsePosList.append(FP)\n",
    "                        falseNegList.append(FN)\n",
    "                        predictions.append(y_pred)\n",
    "                        rseed.append(r)\n",
    "                        slist.append(split)\n",
    "                        wlist.append(week)\n",
    "\n",
    "            resultsDF = pd.DataFrame()\n",
    "            resultsDF[\"week\"] = wlist\n",
    "            resultsDF[\"label\"] = llist\n",
    "            resultsDF[\"split\"] = slist\n",
    "            resultsDF[\"Engineering\"] = flist\n",
    "            resultsDF[\"model\"] = mlist\n",
    "            resultsDF[\"nFeatures\"] = featureList\n",
    "            resultsDF[\"F1\"] = f1List\n",
    "            resultsDF[\"Accuracy\"] = accuracyList\n",
    "            resultsDF[\"truePos\"] = truePosList\n",
    "            resultsDF[\"trueNeg\"] = trueNegList\n",
    "            resultsDF[\"falsePos\"] = falsePosList\n",
    "            resultsDF[\"falseNeg\"] = falseNegList\n",
    "            resultsDF[\"predictions\"] = predictions\n",
    "            resultsDF[\"randomSeed\"] = rseed\n",
    "            # foldername for output\n",
    "            resultsDF.to_csv(drive_base + \"DepreST-CAT-main/machineLearning/screeningResultsGPT/resultsCAT\" + str(week) + \"week\" + label + \"split\" + str(split) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vJ-BGoqgNR0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
